{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Librairies"
      ],
      "metadata": {
        "id": "N8NFcXEH0eHw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pingouin"
      ],
      "metadata": {
        "id": "ZTCF5vXvTvJi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5321195d-5cf3-4114-d408-0f4dbc2550c3"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pingouin in /usr/local/lib/python3.9/dist-packages (0.5.3)\n",
            "Requirement already satisfied: seaborn>=0.11 in /usr/local/lib/python3.9/dist-packages (from pingouin) (0.12.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from pingouin) (1.2.2)\n",
            "Requirement already satisfied: outdated in /usr/local/lib/python3.9/dist-packages (from pingouin) (0.2.2)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.9/dist-packages (from pingouin) (0.8.10)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.9/dist-packages (from pingouin) (1.10.1)\n",
            "Requirement already satisfied: matplotlib>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from pingouin) (3.7.1)\n",
            "Requirement already satisfied: statsmodels>=0.13 in /usr/local/lib/python3.9/dist-packages (from pingouin) (0.13.5)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.9/dist-packages (from pingouin) (1.5.3)\n",
            "Requirement already satisfied: pandas-flavor>=0.2.0 in /usr/local/lib/python3.9/dist-packages (from pingouin) (0.5.0)\n",
            "Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.9/dist-packages (from pingouin) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.0.2->pingouin) (23.1)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.0.2->pingouin) (5.12.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.0.2->pingouin) (3.0.9)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.0.2->pingouin) (4.39.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.0.2->pingouin) (1.0.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.0.2->pingouin) (8.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.0.2->pingouin) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.0.2->pingouin) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.0.2->pingouin) (1.4.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.0->pingouin) (2022.7.1)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.9/dist-packages (from pandas-flavor>=0.2.0->pingouin) (0.2)\n",
            "Requirement already satisfied: xarray in /usr/local/lib/python3.9/dist-packages (from pandas-flavor>=0.2.0->pingouin) (2022.12.0)\n",
            "Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.9/dist-packages (from statsmodels>=0.13->pingouin) (0.5.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from outdated->pingouin) (2.27.1)\n",
            "Requirement already satisfied: littleutils in /usr/local/lib/python3.9/dist-packages (from outdated->pingouin) (0.2.2)\n",
            "Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.9/dist-packages (from outdated->pingouin) (67.6.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->pingouin) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->pingouin) (3.1.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib>=3.0.2->pingouin) (3.15.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from patsy>=0.5.2->statsmodels>=0.13->pingouin) (1.16.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->outdated->pingouin) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->outdated->pingouin) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->outdated->pingouin) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->outdated->pingouin) (2.0.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import pingouin as pg\n",
        "import plotly.express as px\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "from math import ceil\n",
        "sns.set_theme()\n",
        "pd.set_option('display.width', 1000)\n",
        "# Define the tickers you want to get data for\n",
        "tickers = ['CL=F', '^STOXX50E', '^TNX', '^GSPC', 'NG=F']\n",
        "\n",
        "# Define the start and end dates for the data\n",
        "start_date = '2017-01-01'\n",
        "end_date = '2022-03-23'\n",
        "\n",
        "returns = {\n",
        "  '1d_return' : -1,\n",
        "  '5d_return' : -5,\n",
        "  '30d_return' : -30\n",
        "    \n",
        "}\n",
        "output_names = list(returns.keys())\n"
      ],
      "metadata": {
        "id": "0PWlFaWjzaAu"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data acquisition"
      ],
      "metadata": {
        "id": "RiEfwNAd1dHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use yfinance to get the adjusted price for every tickers\n",
        "data = yf.download(tickers, start=start_date, end=end_date).loc[:, 'Adj Close'].dropna()\n",
        "\n",
        "file_path = 'https://raw.githubusercontent.com/CossonArthur/Gas-price-prediction-Statistic/main/NGAS_TS%20-%20Historical%20Data.csv'\n",
        "data_NGAS = pd.read_csv(file_path, parse_dates=['Date']).set_index('Date')\n",
        "data_NGAS.rename(columns={'Price': 'NGAS.L'}, inplace=True)"
      ],
      "metadata": {
        "id": "fuBSRgIVbeO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.merge(data_NGAS, left_index=True, right_index=True, how='left')\n",
        "tickers.append('NGAS.L')"
      ],
      "metadata": {
        "id": "bXpNDiIzheHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We selected a mix of macro, equities and fixed income variables. Eurostoxx and SP500 are the two main equity indices for Europe and US. TNX instead is the reference yield of the US 10 years government bond. The fixed income market is important for commodities pricing as discounting inflation expectations and market risk sentiment. The time series are available in the price domain, so we had to transform into returns at various frequencies. Given the nature of the asset class, we assumed a fixed duration for the 10 year government bond and used a first order linear approximation to estimate returns. For each time series we also calculated a momentum signal through trend filtering. We used for this a simple common used method, which is the difference of two exponentially weighted moving averages with different half lives. This is a simple proxy of the time series gradient and the method is called \"moving average crossover\".\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "23H21JrF3J0K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above code: \n",
        "\n",
        "1.   CL=F is the ticker symbol for crude oil futures\n",
        "2.   ^STOXX50E is the ticker symbol for the Euro\n",
        "3.   Stoxx 50 index\n",
        "4.   ^TNX is the ticker symbol for the 10-year US Treasury yield\n",
        "5. ^GSPC is the ticker symbol for the S&P 500 index\n"
      ],
      "metadata": {
        "id": "OOnVYviRzsYr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filling missing data"
      ],
      "metadata": {
        "id": "MjJfFcYq0XzT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# here we check for data missing\n",
        "dates_check = data.reset_index()['Date']"
      ],
      "metadata": {
        "id": "wAPwF9ZMzjdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dates_check['check'] =  dates_check.diff()\n",
        "dates_check['check'].max()"
      ],
      "metadata": {
        "id": "n1-CDHB0zkRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Features calculation\n",
        "EMAS, MOMENTUM AND RETURNS\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "q-3y_sVL0tDa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the EMAs with half-life of 5, 10, and 30 days\n",
        "for ticker in tickers:\n",
        "    if(ticker != 'NGAS.L'):\n",
        "      data[ticker + '_ema_5'] = data[ticker].ewm(halflife=5).mean()\n",
        "      data[ticker + '_ema_10'] = data[ticker].ewm(halflife=10).mean()\n",
        "      data[ticker + '_ema_30'] = data[ticker].ewm(halflife=30).mean()\n",
        "\n",
        "# Calculate fast and slow momentum for each ticker\n",
        "for ticker in tickers:\n",
        "    if(ticker != 'NGAS.L'):\n",
        "      data[ticker + '_fast_momentum'] = data[ticker + '_ema_5'] - data[ticker + '_ema_10']\n",
        "      data[ticker + '_slow_momentum'] = data[ticker + '_ema_10'] - data[ticker + '_ema_30']\n",
        "\n",
        "# Calculate daily, 5-day, and 30-day rolling returns for each ticker\n",
        "# TODO : handle the creation of data by shrinking the interval by 30 day \n",
        "for ticker in tickers:\n",
        "    if ticker == '^TNX':\n",
        "        # Calculate returns for ^TNX with a different formula as this is a bond\n",
        "        for key, val in returns.items():\n",
        "          data[ticker + '_' + key] = (data[ticker] - data[ticker].shift(val))/100 * -7 \n",
        "    else:\n",
        "        #https://www.wikiwand.com/en/Rate_of_return#Logarithmic_or_continuously_compounded_return\n",
        "        for key, val in returns.items():\n",
        "          data[ticker + '_' + key] = np.log(data[ticker] / data[ticker].shift(val))\n",
        "\n",
        "data.dropna(inplace = True)"
      ],
      "metadata": {
        "id": "UJ3lL6770wA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data analysis"
      ],
      "metadata": {
        "id": "P8_T7wwcyc5_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(ceil(len(tickers)/3),3,figsize=(12*ceil(len(tickers)//3), 10),sharex='all')\n",
        "for i in range(len(tickers)):\n",
        "  for x in output_names:\n",
        "    axs[i//3,i%3].plot(data[tickers[i] + '_' + x])\n",
        "  axs[i//3,i%3].legend(output_names)\n",
        "  axs[i//3,i%3].set_title('Return for : {}'.format(tickers[i]))\n"
      ],
      "metadata": {
        "id": "TY8LB4BHA9Fn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Time series seasonnal decomposition"
      ],
      "metadata": {
        "id": "eY8arX6uZ4C8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import statsmodels.api as sm\n",
        "\n",
        "for x in output_names:\n",
        "  decomposition = sm.tsa.seasonal_decompose(data['NGAS.L_' + x], model = 'additive',period=1)\n",
        "  decomposition.plot()"
      ],
      "metadata": {
        "id": "TxOgv6c6Z7PX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Correlation analysis"
      ],
      "metadata": {
        "id": "IyVXooLTynEb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#result = pg.pairwise_corr(data)\n",
        "#print(result)\n",
        "\n",
        "#print(data.pcorr().round(3))\n",
        "\n",
        "df = data.rcorr(decimals=2,upper='n',method='spearman').replace(['-',data.shape[0]],0).astype('float')\n",
        "px.imshow(df)\n",
        "\n"
      ],
      "metadata": {
        "id": "2PwXaAfr1Wc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normality test"
      ],
      "metadata": {
        "id": "056YMVA26yG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = pg.normality(data)\n",
        "print(result)\n",
        "#ax = pg.qqplot(data['CL=F_5d_return'], dist='norm')\n",
        "ax = pg.qqplot(data['CL=F_5d_return'], dist='norm')"
      ],
      "metadata": {
        "id": "d55omRLi6z7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot"
      ],
      "metadata": {
        "id": "RTdOW6Tr1s_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the columns with '5d_return' in the name, excluding 'NGAS.L_5d_return'\n",
        "cols_5d_return = [col for col in data.columns if '5d_return' in col and col != 'NGAS.L_5d_return']\n",
        "\n",
        "# Calculate the rolling correlation with a window of 30 days for each column against 'NGAS.L_5d_return'\n",
        "rolling_correlations = data[cols_5d_return + ['NGAS.L_5d_return']].rolling(window=30).corr()\n",
        "\n",
        "# Determine the number of subplots required\n",
        "num_subplots = len(cols_5d_return)\n",
        "\n",
        "# Set up the subplots with the desired size\n",
        "fig, axes = plt.subplots(num_subplots, 1, figsize=(8, 4 * num_subplots), sharex=True)\n",
        "\n",
        "# Plot each rolling correlation in a separate subplot\n",
        "for idx, col in enumerate(cols_5d_return):\n",
        "    rolling_correlations.loc[pd.IndexSlice[:, col], 'NGAS.L_5d_return'].plot(ax=axes[idx])\n",
        "    axes[idx].set_title(f'Rolling 30-Day Correlation of {col} with NGAS.L_5d_return')\n",
        "    axes[idx].set_ylabel('Correlation')\n",
        "\n",
        "plt.xlabel('Date')\n",
        "plt.tight_layout()\n",
        "\n"
      ],
      "metadata": {
        "id": "XYx9fhrV1vfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## PCA\n"
      ],
      "metadata": {
        "id": "dpBjWzmphAoa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_columns = [col for col in data.columns if not('NGAS' in col)]\n",
        "X = data[input_columns]\n",
        "\n",
        "print('Number of initial features : {}'.format(len(input_columns)))"
      ],
      "metadata": {
        "id": "xMA-tDLmBODH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X)\n",
        "X = scaler.transform(X)\n",
        "\n",
        "pca = PCA()\n",
        "X_hat = pca.fit_transform(X)"
      ],
      "metadata": {
        "id": "efafSm3X2-dc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prop_var = pca.explained_variance_ratio_\n",
        "\n",
        "fig, axs = plt.subplots(1,2,figsize=(8,4))\n",
        "axs[0].plot(range(1,len(prop_var)+1),np.cumsum(prop_var), 'ro-')\n",
        "axs[0].set_xlabel('Features', fontsize=8)\n",
        "axs[0].set_ylabel('Cumulative Explained Variance', fontsize=8)\n",
        "axs[1].plot(range(1,len(prop_var)+1),pca.explained_variance_,'bo-')\n",
        "axs[1].set_ylabel('Explained Variance', fontsize=8)\n",
        "axs[1].set_xlabel('Features', fontsize=8)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "U51gGRShit-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def biplot(score,coef,labels=None,represent = None):\n",
        "  \n",
        "    if (represent != None):\n",
        "       labels = [ticker if ticker in represent else ' ' for ticker in labels]\n",
        "\n",
        "    xs = score[:,0]\n",
        "    ys = score[:,1]\n",
        "    n = coef.shape[0]\n",
        "    scalex = 1.0/(xs.max() - xs.min())\n",
        "    scaley = 1.0/(ys.max() - ys.min())\n",
        "    plt.scatter(xs * scalex,ys * scaley,\n",
        "                s=5, \n",
        "                color='orange')\n",
        " \n",
        "    for i in range(n):\n",
        "        plt.arrow(0, 0, coef[i,0], \n",
        "                  coef[i,1],color = 'purple',\n",
        "                  alpha = 0.5)\n",
        "        plt.text(coef[i,0]* 1.15, \n",
        "                 coef[i,1] * 1.15, \n",
        "                 labels[i], \n",
        "                 color = 'darkblue', \n",
        "                 ha = 'center', \n",
        "                 va = 'center')\n",
        " \n",
        "    plt.xlabel(\"PC{}\".format(1))\n",
        "    plt.ylabel(\"PC{}\".format(2))    \n",
        " \n",
        " \n",
        "    plt.figure()"
      ],
      "metadata": {
        "id": "hHl4S35A3DPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title('Biplot of PCA')\n",
        " \n",
        "biplot(X_hat, np.transpose(pca.components_), input_columns,tickers)"
      ],
      "metadata": {
        "id": "GGg1eW3M3STS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Principal components selection"
      ],
      "metadata": {
        "id": "yGI1W1YUGnWz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title PAC limit choice\n",
        "max_component = 12 #@param {type:\"slider\", min:0, max:45, step:1}\n",
        "\n",
        "\n",
        "print('Amount of variance explained by the choice of principal components : {}'.format(round(np.sum(prop_var[:max_component+1]),3)))\n",
        "\n",
        "weights = pd.DataFrame(np.transpose(pca.components_), index=input_columns)\n",
        "fig = px.bar(weights.loc[:,:max_component],title=\"Weight\")\n",
        "fig.show() # double clicked on a component in the legend to isolate\n"
      ],
      "metadata": {
        "id": "aTbq5ga2JWz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification models \n",
        "\n"
      ],
      "metadata": {
        "id": "Vf6uUFWg5qBc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data preprocessing"
      ],
      "metadata": {
        "id": "qbCR1mhCaX41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# input vector\n",
        "X_hat_reduced = X_hat[:-1,:max_component] #shift the input to predict t with t-1 and keep only the principals components"
      ],
      "metadata": {
        "id": "cGcasxc2GqYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "Y = data[[col for col in data.columns if ('NGAS' in col)]].iloc[1:] # target vector shift the input to predict t with t-1\n",
        "Y_class =  pd.DataFrame() # target vector for class \n",
        "\n",
        "\n",
        "for x in output_names:\n",
        "  Y_class[x] = np.where(Y['NGAS.L_' + x] >= Y['NGAS.L_' + x].quantile(0.75),'High Profit', \n",
        "                        np.where(Y['NGAS.L_' + x] >= Y['NGAS.L_' + x].quantile(0.5),'Marginal Profit',     \n",
        "                            np.where(Y['NGAS.L_' + x] >= Y['NGAS.L_' + x].quantile(0.25),'Marginal Loss', 'High loss')\n",
        "                        )\n",
        "              )\n",
        "\n",
        "for x in output_names:\n",
        "  Y_class[x] = np.where(Y['NGAS.L_' + x] >= Y['NGAS.L_' + x].std(),'High Profit', \n",
        "                        np.where(Y['NGAS.L_' + x] >= 0,'Marginal Profit',     \n",
        "                            np.where(Y['NGAS.L_' + x] >= -Y['NGAS.L_' + x].std(),'Marginal Loss', 'High loss')\n",
        "                        )\n",
        "              )\n",
        "\n",
        "print(pd.concat([Y_class[x].value_counts() for x in output_names],axis=1))\n",
        "\n",
        "# Encode target classes as numerical labels\n",
        "le = LabelEncoder()\n",
        "Y_class_encoded = np.transpose(np.array([le.fit_transform(Y_class[x]) for x in Y_class.columns]))\n",
        "target_names = le.classes_\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, Y_train_class, Y_test_class = train_test_split(X_hat_reduced, Y_class_encoded, test_size=0.2, random_state=17)\n"
      ],
      "metadata": {
        "cellView": "code",
        "id": "YGMgD2lsnGt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction "
      ],
      "metadata": {
        "id": "ZiojZVWzSyP6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forest"
      ],
      "metadata": {
        "id": "_jDJEXM4ky-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "# Define class weights\n",
        "class_weights = {0: 0.5,1: 1, 2: 0.5, 3: 1}\n",
        "\n",
        "for i in range(Y_class.shape[1]):\n",
        "  # Train a random forest classifier on the training set\n",
        "  rfc = RandomForestClassifier(n_estimators=300, random_state=42, class_weight=class_weights)\n",
        "  rfc.fit(X_train, Y_train_class[:,i])\n",
        "\n",
        "  # Make predictions on the testing set\n",
        "  Y_pred = rfc.predict(X_test)\n",
        "\n",
        "  # Print classification report\n",
        "  print(output_names[i])\n",
        "  print('-'*60)\n",
        "  print(classification_report(Y_test_class[:,i], Y_pred, target_names=target_names))\n",
        "  print('='*60)\n",
        "\n"
      ],
      "metadata": {
        "id": "fl9DcWYteiHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Metrics and definition** \n",
        "* **Precision** ->\tPrecision is defined as the ratio of true positives to the sum of true and false positives.\n",
        "* **Recall** ->\tRecall is defined as the ratio of true positives to the sum of true positives and false negatives.\n",
        "* **F1 Score**\t-> The F1 is the weighted harmonic mean of precision and recall. The closer the value of the F1 score is to 1.0, the better the expected performance of the model is.\n",
        "* **Support** ->\tSupport is the number of actual occurrences of the class in the dataset. It doesnâ€™t vary between models, it just diagnoses the performance evaluation process.\n"
      ],
      "metadata": {
        "id": "bdTJxJ3EMJP6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Probabilistic discriminative approach: Logistic Regression\n",
        "\n",
        "Let us change the methods for the classification task and use a Logistic regression classifier with two classes:\n",
        "- Hypothesis space: $y_n = y(x_n) = \\sigma(w_0 + x_{n1} w_1 + x_{n2} w_2)$;\n",
        "- Loss measure: Loglikelihood $L(\\mathbf{w}) = -\\sum_{n=1}^N  [C_n \\ln y_n + (1 - C_n) \\ln (1 - y_n)]$;\n",
        "- Optimization method: Gradient Descent;\n",
        "\n",
        "where the sigmoid function is defined as $\\sigma(x) = \\frac{1}{1 + e^{-x}}$.\n",
        "* Pros:\n",
        "  * Can be simpler and faster to train, since it only needs to estimate the conditional distribution of the labels given the features.\n",
        "  * May be more effective at capturing the decision boundaries between classes, since it focuses on modeling the conditional distribution of the labels.\n",
        "  * Can be more interpretable, since it provides a clear probabilistic interpretation of the classification decision.\n",
        "* Cons:\n",
        "  * Can be less robust to missing data and noisy data, since it only models the conditional distribution of the labels given the features.\n",
        "  * May be more prone to overfitting, since it only models the conditional distribution of the labels given the features and may not capture the full distribution of the data.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZXyCS_O41ZcR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "for i in range(Y_class.shape[1]):\n",
        "  # Train a Logistic Regression classifier on the training set\n",
        "  log_classifier = LogisticRegression(penalty=None) # regularization is applied as default\n",
        "  log_classifier.fit(X_train, Y_train_class[:,i])\n",
        "\n",
        "  # Make predictions on the testing set\n",
        "  Y_pred = log_classifier.predict(X_test)\n",
        "\n",
        "  # Print classification report\n",
        "  print(output_names[i])\n",
        "  print('-'*60)\n",
        "  print(classification_report(Y_test_class[:,i], Y_pred, target_names=target_names))\n",
        "  print('='*60)"
      ],
      "metadata": {
        "id": "Xj9A-3rh38r2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Probabilistic generative approach: Naive Bayes\n",
        "\n",
        "\n",
        "Generative models have the purpose of modeling the joint pdf of the couple input/output $p(C_k,\\mathbf{x})$, which allows us to generate also **new data** from what we learned.\n",
        "\n",
        "This is different from the probabilistic discriminative models, in which we are only interested in computing the probabilities that a given input is coming from a specific class $p(C_k | \\mathbf{x})$, which is not sufficient to produce new samples.\n",
        "\n",
        "Conversely, we will see how it is possible to generate new samples if we are provided with an approximation of the joint input/output distribution $p(C_k,\\mathbf{x})$.\n",
        "\n",
        "In this case, the Naive Bayes method considers the **naive assumption** that each input is conditionally (w.r.t. the class) independent from each other.\n",
        "If we consider the Bayes formula we have: \n",
        "\\begin{align*}\n",
        "\t& p(C_k | \\mathbf{x}) = \\frac{p(C_k) \\ p(\\mathbf{x} | C_k)}{p(\\mathbf{x})} \\\\\n",
        "\t& \\propto p(x_1, \\ldots, x_M, C_k)\\\\\n",
        "\t& = p(x_1 | x_2, \\ldots, x_M, C_k) p(x_2, \\ldots, x_M, C_k) \\\\\n",
        "\t& = p(x_1 | x_2, \\ldots, x_M, C_k) p(x_2 | x_3, \\ldots, x_M, C_k) p(x_3, \\ldots, x_n, C_k) \\\\\n",
        "\t& = p(x_1 \\vert x_2, \\ldots, x_M, C_k) \\ldots p(x_M | C_k) p(C_k) \\\\\n",
        "\t& = p(x_1 \\vert C_k) \\ldots p(x_M | C_k) p(C_k) \\\\\n",
        "\t& = p(C_k) \\prod_{j=1}^M p(x_j | C_k).\n",
        "\\end{align*}\n",
        "\n",
        "The decision function, which maximises the Maximum A Posteriori probability, is the following:\n",
        "\\begin{equation*}\n",
        "\ty(\\mathbf{x}) = \\arg \\max_k p(C_k) \\prod_{j=1}^M p(x_j | C_k),\n",
        "\\end{equation*}\n",
        "where as usual we do not consider the normalization factor $p(\\mathbf{x})$.\n",
        "\n",
        "In a specific case we have to define a prior distribution for the classes $p(C_k) \\ \\forall k$ and a distribution to compute the likelihood of the considered samples $p(x_j | C_k) \\ \\forall J, \\ \\forall k$.\n",
        "\n",
        "In the case of continuous variable one of the usual assumption is to use Gaussian distributions for each variable $p(x_j | C_k) = \\mathcal{N}(x_j;\\mu_{jk},\\sigma^2_{jk})$ and either a uniform prior $p(C_k) = \\frac{1}{K}$ or a multinomial prior based on the samples proportions $p(C_k) = \\frac{\\sum_{i=1}^N I \\{\\mathbf{x}_n \\in C_k \\}}{N}$, where $I\\{\\cdot\\}$ is the indicator function.\n",
        "\n",
        "The complete model of Naive Bayes is:\n",
        "- Hypothesis space: $y_n = y(x_n) = \\arg \\max_k p(C_k) \\prod_{j=1}^M p(x_j | C_k)$;\n",
        "- Loss measure: Log likelihood;\n",
        "- Optimization method: MLE.\n",
        "\n",
        "* Pros:\n",
        " * Can handle missing data and noisy data well, since it models the joint distribution of features and labels.\n",
        "  * Can be more robust to overfitting, since it estimates the full distribution of the data rather than just the conditional distribution of the labels.\n",
        "  * Can be useful for generating synthetic data that follows the same distribution as the training data.\n",
        "* Cons:\n",
        "  * Can be computationally more expensive, since it involves estimating the joint distribution.\n",
        "  * Can suffer from the curse of dimensionality when dealing with high-dimensional feature spaces.\n",
        "  * May not be as good at capturing the subtle decision boundaries between classes, since it models the distribution of the data as a whole rather than just the boundaries between classes.\n",
        " "
      ],
      "metadata": {
        "id": "loiHY3uI1zmj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "for i in range(Y_class.shape[1]):\n",
        "\n",
        "  # Train a Naive Bayes classifier on the training set\n",
        "  gnb_classifier = GaussianNB()\n",
        "  gnb_classifier.fit(X_train, Y_train_class[:,i])\n",
        "\n",
        "  # Make predictions on the testing set\n",
        "  Y_pred = gnb_classifier.predict(X_test)\n",
        "\n",
        "  # Print classification report\n",
        "  print(output_names[i])\n",
        "  print('-'*60)\n",
        "  print(classification_report(Y_test_class[:,i], Y_pred, target_names=target_names))\n",
        "  print('='*60)"
      ],
      "metadata": {
        "id": "thAFaLdw5XwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ovrersampling (meh)"
      ],
      "metadata": {
        "id": "Q8EuCRNyJwYM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "# Define the oversampling strategy\n",
        "oversample = RandomOverSampler(sampling_strategy='minority')\n",
        "\n",
        "# Fit and apply the oversampling to the training data\n",
        "X_train_oversampled, y_train_oversampled = oversample.fit_resample(X_train, Y_train_class)"
      ],
      "metadata": {
        "id": "3wQityQsJw5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Encode target classes as numerical labels\n",
        "le = LabelEncoder()\n",
        "target_class_encoded = le.fit_transform(target_class ['30d'])\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(pca_20_features, target_class_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Naive Bayes classifier on the training set\n",
        "gnb_classifier = GaussianNB()\n",
        "gnb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred = gnb_classifier.predict(X_test)\n",
        "\n",
        "# Print classification report\n",
        "target_names = le.classes_\n",
        "print(classification_report(y_test, y_pred, target_names=target_names))"
      ],
      "metadata": {
        "id": "z7gtv-xlJ5QZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Encode target classes as numerical labels\n",
        "le = LabelEncoder()\n",
        "target_class_encoded = le.fit_transform(target_class ['30d'])\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(pca_20_features, target_class_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Naive Bayes classifier on the training set\n",
        "gnb_classifier = GaussianNB()\n",
        "gnb_classifier.fit(X_train_oversampled, y_train_oversampled)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred = gnb_classifier.predict(X_test)\n",
        "\n",
        "# Print classification report\n",
        "target_names = le.classes_\n",
        "print(classification_report(y_test, y_pred, target_names=target_names))"
      ],
      "metadata": {
        "id": "45LhPJ5SJ7J2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}